{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "8acbc523",
   "metadata": {},
   "source": [
    "# Exploring Stochastic Gradient Descent with Pytorch: A Simple Linear Approximation Example\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this Jupyter notebook exploring the power of Stochasting Gradient Descent, the workhorse of machine learning, and its implementation using Pytorch. \n",
    "\n",
    "Our objective of this notebook is to provide a clear and intuitive understanding of how [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) works by solving a simple linear approximation problem using gradient descent, and how to code it using [PyTorch](https://pytorch.org), one of the most popular deep learning frameworks.\n",
    "\n",
    "\n",
    "### A Simplified Version of a Neural Network Architecture\n",
    "\n",
    "The motivation for creating this notebook is to gain hands-on experience in coding with PyTorch. The simplicity of the linear model will allow us to easily verify and visualize our results, and also allows us to focus on the optimization process and understand the workings of the algorithm better.\n",
    "\n",
    "In addition to core Pytorch utilities such as `nn.Module` to define our linear model, we will also explore some of the additional modules such as an optimizer to update the model parameters, and a scheduler to reduce the learning rate. We use Pytorch's data classes to wrap the data for training and evaluation and use a generator to control the order of sampling training data. Finally, we will prepare to leverage PyTorch's ability to use [CUDA](https://pytorch.org/docs/stable/notes/cuda.html) GPUs to accelerate the learning process in more compliated machine learning tasks.\n",
    "\n",
    "While the linear model implemented in this notebook is a relatively simple example, the architecture and code structure used is very similar to what would be used in a real-world machine learning problem. In fact, this implementation can be easily extended to more complex neural network architectures simply by adding additional layers and modifying the loss function and optimizer. Thus, the concepts and techniques presented here are highly relevant to anyone interested in machine learning with PyTorch.\n",
    "\n",
    "### Visualizing the SGD Algorithm\n",
    "\n",
    "One of the goals of this notebook is to gain a better understanding of how the stochastic gradient descent (SGD) algorithm works. In this notebook, we will not only implement SGD using Pytorch but also provide visualizations that show how the algorithm converges towards the optimal value. We will show how the error surface changes with each iteration when a new data sample is used to calculate the value of the loss function and, consequently, how the gradient points to current direction of steepest descent.\n",
    "\n",
    "By visualizing these changes, we hope to gain insight into how the algorithm is able to find the optimal values for the model parameters. The principles we will be exploring can be extended to more complex problems and models.\n",
    "\n",
    "### Use of ChatGPT for Text Generation\n",
    "\n",
    "While the code has been manually crafted by the author, the texts in this notebook, including this introduction, have been created with the aid of [ChatGPT](https://openai.com/blog/chatgpt), a large language model trained by [OpenAI](https://openai.com/). By using prompts, ChatGPT generates human-like text based on the input it receives.\n",
    "\n",
    "The language model generated each piece of text in response to a prompt designed to elicit a specific type of response. For example, a prompt may request a subheading and a description of a code cell. These prompts were created by the author who then organized the generated text into a coherent and meaningful narrative. Also, the LaTeX equations presented in this document were created with the assistance of ChatGPT.\n",
    "\n",
    "For example, a prompt could be \"_Can you provide a brief explanation of the cubehelix_palette colormap in Seaborn?_\" and the resulting text chapter would be a concise explanation of the colormap.\n",
    "\n",
    "Here's another example of a prompt that was used to create a descriptive subheading and text chapter based on the code cell contents.\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We hope that this notebook will serve as a useful resource for those who are new to PyTorch, as well as those who are looking to improve their understanding of linear regression and gradient descent. So, whether you're new to machine learning or an experienced practitioner, this notebook will provide you with valuable insights into the workings of SGD and its implementation using Pytorch.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bcb423",
   "metadata": {},
   "source": [
    "## Introduction to the Approximation Problem and Linear Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bdc9906",
   "metadata": {},
   "source": [
    "The function we aim to approximate in this notebook is a fourth-degree polynomial:\n",
    "\n",
    "$$\n",
    "f(x) = - 0.4 x^4  + 1.8 x^3 - 2 x + 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x_data):\n",
    "    return - 0.4 * x_data ** 4 + 1.8 * x_data ** 3 - 2 * x_data + 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4ecfab0",
   "metadata": {},
   "source": [
    "We will add gaussian $\\mathcal{N}(0,1)$ noise to the observations and approximate the polynomial function using a linear model. Specifically, we will try to learn the coefficients of a linear equation of the form:\n",
    "\n",
    "$$y = wx + b$$\n",
    "\n",
    "where $w$ is the weight, $b$ is the bias, and $x$ is the input feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78107bf2",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6956052",
   "metadata": {},
   "source": [
    " We use gradient descent to iteratively adjust the values of $w$ and $b$ in order to minimize the difference between the observed noisy values of the polynomial and the values predicted by the linear model. \n",
    " \n",
    " Note that the term \"stochastic\" refers to the fact that the algorithm selects a random subset of the training data to compute the gradient of the loss function and update the model parameters in each iteration, rather than using the entire dataset.\n",
    " \n",
    " The use of training batches makes the computation of the gradients and the update of the parameters faster and more computationally efficient, especially for large datasets. However, since the training batch is randomly selected, the gradient estimate is a stochastic approximation of the true gradient, which introduces some noise and randomness into the optimization process.\n",
    "\n",
    " In the following, we define the SGD updates for our linear model using [mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) as loss function. MSE loss is a common loss function for regression problems and measures the average squared difference between the predicted and actual values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eed3b7e",
   "metadata": {},
   "source": [
    "The mean squared error (MSE) loss function measures the average squared difference between the predicted and true values for all samples in the batch. The mathematical equation for the MSE loss function can be expressed as:\n",
    "\n",
    "$$ \\mathcal{L}_{MSE} = \\frac{1}{N}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "where $N$ is the number of samples in the batch, $y_i$ is the true value for sample $i$, and $\\hat{y}_i$ is the predicted value for sample $i$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c117b5ec",
   "metadata": {},
   "source": [
    "When $\\hat{y}_i$ is estimated by linear model $\\hat{y}_i=wx_i+b$, the equation becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{MSE} = \\frac{1}{N}\\sum_{i=1}^N (y_i - (w x_i + b))^2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c206846",
   "metadata": {},
   "source": [
    "The equation for the partial derivative of the MSE loss with respect to the weight parameter $w$, when using linear estimation, can be derived as follows:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^{N} 2(y_i - (wx_i + b))(-x_i)$$\n",
    "\n",
    "Rearranging slightly, we get:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial w} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (wx_i + b))x_i$$\n",
    "\n",
    "where $N$ is the number of samples in the batch, $x_i$ is the $i$-th feature value, and $y_i$ is the corresponding target value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "079dd9d6",
   "metadata": {},
   "source": [
    "Similarly, the equation for the partial derivative with respect to the bias parameter $b$ is given by\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}_{MSE}}{\\partial b} = -\\frac{2}{N} \\sum_{i=1}^{N} (y_i - (wx_i + b))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a648510",
   "metadata": {},
   "source": [
    "The logic of the SGD algorithm is to update the parameter values at each iteration in the direction of the negative gradient of the loss function with respect to the parameters. This approach enables the algorithm to converge towards the optimal parameter values that minimize the loss function. The _gradient descent update_ for weight $w$ and bias $b$ with learning rate $\\alpha$ is given by:\n",
    "\n",
    "$$w_{i+1} = w_i - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w}\\bigg\\rvert_{w=w_i}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$b_{i+1} = b_i - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\\bigg\\rvert_{w=w_i}$$\n",
    "\n",
    "where $i$ is the iteration index, and notation $\\frac{\\partial \\mathcal{L}}{\\partial w}\\bigg\\rvert_{w=w_i}$ represents the partial derivative of the loss function with respect to the weight parameter $w$, evaluated at the current weight value $w_i$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdcb45ef",
   "metadata": {},
   "source": [
    "## Library imports\n",
    "\n",
    "First, we import the necessary libraries for our Pytorch implementation of the SGD algorithm. As discussed, we will use [Pytorch](https://pytorch.org/) to define and train our linear model. We will also use [NumPy](https://numpy.org/) and [Pandas](https://pandas.pydata.org/) for data handling, and [Matplotlib](https://matplotlib.org/) and [Seaborn](https://seaborn.pydata.org/) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf17e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4877c1d",
   "metadata": {},
   "source": [
    "We confirm the version of Pytorch used as [version 2.0](https://pytorch.org/blog/pytorch-2.0-release/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b82d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ced39134",
   "metadata": {},
   "source": [
    "## Check for GPU support\n",
    "\n",
    "We check for the availability of a CUDA-enabled GPU and set the device to use it for our Pytorch implementation. Even if the current GPU turns out to be a modest NVIDIA GeForce GTX 960, using a GPU can significantly speed up the computation time and improve the training efficiency in a more realistic setting. We will use the Pytorch [torch.cuda](https://pytorch.org/docs/stable/cuda.html) library to check for the availability of a GPU and set the device to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a603b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5539127",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5098455",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"NA\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb53b939",
   "metadata": {},
   "source": [
    " ### Define colors and styles for visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2464ac5",
   "metadata": {},
   "source": [
    "We have selected to use the [RdYlBu colormap](https://colorbrewer2.org/#type=diverging&scheme=RdYlBu&n=7) colors for our visualizations. This 7-class diverging color palette from the ColorBrewer scheme ranges from red to blue with a yellow color at the center. The palette distributes the colors in a way that emphasizes a midpoint or neutral value, with lighter shades towards the center and darker shades towards the edges of the color spectrum.\n",
    "\n",
    "Later in the code, we refer to the colors as `colors['red']`, `colors['orange']`, and so on, instead of default color names. We assign the corresponding colors in the palette to these names using the zip() function to combine the color names and palette colors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ba2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RdYlBu = sns.color_palette(\"RdYlBu\", 7)\n",
    "sns.palplot(RdYlBu)\n",
    "\n",
    "colors = dict(zip(['red', 'orange', 'lightorange', 'yellow',\n",
    "              'lightblue', 'steelblue', 'blue'],\n",
    "                  RdYlBu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cafe65",
   "metadata": {},
   "source": [
    "The flare color palette, instantiated as `sns.color_palette(\"flare_r\", as_cmap=True)`, is a colormap consisting of a range of bright and vibrant colors that shift from yellow to red, with darker shades indicating higher values and lighter shades indicating lower values. The colors in the palette are designed to give the impression of a \"flare\" or explosion of bright colors, making it ideal for highlighting important information or areas of interest in data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5605d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"flare_r\", as_cmap=True)\n",
    "palette"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64cb148e",
   "metadata": {},
   "source": [
    "The cubehelix colormap is a color scheme that was designed to show variations of luminosity and hue in a perceptually uniform way, making it ideal for representing data with continuous variations. It starts from a dark color and gradually lightens as it spirals through a set of hues. As the `as_cmap` parameter is set to True, the output of the function is a matplotlib `colormap` object ([link](https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.Colormap.html)) that can be used to assign colors to data in visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87181219",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.cubehelix_palette(\n",
    "    start=.5, rot=-.75, dark=0.25, light=1,\n",
    "    reverse=True, as_cmap=True)\n",
    "cm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79360f38",
   "metadata": {},
   "source": [
    "We also define the default axis styles and image size below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1661b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_ax(ax):\n",
    "    ax.xaxis.set_tick_params(labelsize=12)\n",
    "    ax.yaxis.set_tick_params(labelsize=12)\n",
    "\n",
    "    plt.setp(ax.spines.values(), linewidth=1.5)\n",
    "\n",
    "\n",
    "FIG_W = 16\n",
    "FIG_H = 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a203ca25",
   "metadata": {},
   "source": [
    "## Random Seed Initialization\n",
    "\n",
    "We set the random seed for reproducibility. By setting the same seed every time the code is run, we ensure that the random initialization is consistent, and any differences in the output are due to changes in the training process rather than the initialization. \n",
    "\n",
    "We set the random seed for Pytorch using `torch.manual_seed()` and for NumPy using `np.random.seed()`. Later, we use `torch.Generator`to seed the generation of samples from our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d691b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 1970\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33b23edb",
   "metadata": {},
   "source": [
    "## Create the data sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64e1e6c7",
   "metadata": {},
   "source": [
    "First, we define constants for our data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SAMPLES = 200\n",
    "EVALUATION_SAMPLES = 50\n",
    "DATA_X_MIN = -1\n",
    "DATA_X_MAX = 3\n",
    "TRAINING_BATCH_SIZE = 1\n",
    "EVALUATION_BATCH_SIZE = EVALUATION_SAMPLES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb3b7aa",
   "metadata": {},
   "source": [
    "### Custom Dataset Class for Creating Training and Evaluation Data\n",
    "\n",
    "This code defines a custom PyTorch [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) subclass `FuncDataset`, which generates training and evaluation data for our linear model using the fourth-degree polynomial with added Gaussian noise.\n",
    "\n",
    "The `len` method returns the length of the data, and the `getitem` method returns a tuple of x and y_noisy data at the given index. Note that these two methods of the abstract `Dataset` class should be overridden by subclass implementations.\n",
    "\n",
    "The `init` method initializes the class using the `create_data` method, in turn invoking the `func` method defined earlier to draw from our polynomial model and adding gaussian noise to the samples. The method uses the given minimum and maximum input values, number of samples, and either random or linear spacing for input values. \n",
    "\n",
    "Note, that in order to ensure that our data is processed by the appropriate device and to take advantage of any available GPU, we make use of the `Tensor.to()` method in PyTorch ([link](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to)). This method uses the `device` variable defined earlier (e.g., CPU or GPU), to set the device on which the data should be stored and processed.\n",
    "\n",
    "When data is on the GPU, it is in a format that is not directly compatible with standard Python libraries such as NumPy and Seaborn. Therefore, we need to move the data from the GPU to the CPU using the `Temsor.to('cpu')` method. The `detach()` function is used to remove the tensor from the computation graph to avoid backpropagation. So, our `as_numpy()` function is used to convert the tensor to a NumPy array for easier visualization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd90d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuncDataset(Dataset):\n",
    "    def __init__(self, DATA_X_MIN, DATA_X_MAX, samples):\n",
    "        self._x, self._y_real, self._y_noisy = self.create_data(\n",
    "            DATA_X_MIN, DATA_X_MAX, samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._x[index], self._y_noisy[index]\n",
    "\n",
    "    def create_data(self, DATA_X_MIN, DATA_X_MAX, samples, x_spacing='Random'):\n",
    "\n",
    "        if x_spacing == 'Random':\n",
    "            x = torch.FloatTensor(samples, 1).uniform_(\n",
    "                DATA_X_MIN, DATA_X_MAX).sort(dim=0)[0].to(device)\n",
    "        else:\n",
    "            x = torch.linspace(DATA_X_MIN, DATA_X_MAX,\n",
    "                               samples).unsqueeze(-1).to(device)\n",
    "\n",
    "        y_real = func(x)\n",
    "        y_noisy = y_real + torch.randn_like(y_real)\n",
    "\n",
    "        return x, y_real, y_noisy\n",
    "\n",
    "    def as_numpy(self):\n",
    "        return (self._x.squeeze()\n",
    "                .to('cpu').detach().numpy(),\n",
    "                self._y_real.squeeze()\n",
    "                .to('cpu').detach().numpy(),\n",
    "                self._y_noisy.squeeze()\n",
    "                .to('cpu').detach().numpy())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "700686b8",
   "metadata": {},
   "source": [
    "### Data Preparation and Dataloader Initialization\n",
    "\n",
    "Next, we focus on preparing the data for training and validation by defining the necessary datasets and dataloaders. On top of datasets, we use `torch.utils.data.DataLoader` ([link](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)). Data loader allows us to conviniently load the data in batches and suffle the data for each training epoch.\n",
    "\n",
    "We define a generator `torch.Generator` ([link](https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator)) and set the generator seed using `gen.manual_seed()` and pass it to our training data loader using the generator parameter. This allows us to recreate the same data sampling order used during training, which is stored in the report returned by our training function. By setting the seeds, we later ensure that the visualizations of gradient and error surface are coherent and reflect the same data order as used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad445418",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator()\n",
    "gen.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3c346f0",
   "metadata": {},
   "source": [
    "\n",
    "We use the `FuncDataset` class defined above to generate the training and evaluation datasets. We then use `DataLoader` to create the corresponding training and evaluation dataloaders. Additionally, we specify the batch sizes for each dataloader, and set the shuffle flag to True for the training dataloader to ensure that samples are presented in random order during each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c750a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = FuncDataset(\n",
    "    DATA_X_MIN, DATA_X_MAX, TRAINING_SAMPLES)\n",
    "training_loader = DataLoader(\n",
    "    training_dataset, batch_size=TRAINING_BATCH_SIZE, shuffle=True,\n",
    "    generator=gen)\n",
    "\n",
    "evaluation_dataset = FuncDataset(\n",
    "    DATA_X_MIN, DATA_X_MAX, EVALUATION_SAMPLES)\n",
    "evaluation_loader = DataLoader(\n",
    "    evaluation_dataset, batch_size=EVALUATION_BATCH_SIZE, shuffle=False)\n",
    "\n",
    "x_array, y_real_array, y_noisy_array = training_dataset.as_numpy()\n",
    "x_eval_array, _, y_eval_noisy_array = evaluation_dataset.as_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7684423",
   "metadata": {},
   "source": [
    "### Data set visualization\n",
    "\n",
    "At this point, it is good to visualize the training and evaluation datasets we just created. The training dataset is shown in blue, where the solid blue line represents the ground truth polynomial function and the blue dots represent the noisy data points. The evaluation dataset is shown in orange, where the orange dots represent the noisy data points.\n",
    "\n",
    "The x-axis represents the input data, and the y-axis represents the output data. The graph is generated using the seaborn library, and the `sns.despine()` function is used to remove the top and right spines of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924eba5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(FIG_W, FIG_H))\n",
    "\n",
    "sns.lineplot(x=x_array, y=y_real_array,\n",
    "             color=colors['steelblue'], linewidth=2)\n",
    "sns.scatterplot(x=x_array, y=y_noisy_array,\n",
    "                color=colors['blue'])\n",
    "sns.scatterplot(x=x_eval_array, y=y_eval_noisy_array,\n",
    "                color=colors['orange'])\n",
    "\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xticks(range(-1, 4, 1))\n",
    "ax.set_yticks(range(0, 17, 1))\n",
    "sns.despine()\n",
    "set_ax(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8aa2b5e1",
   "metadata": {},
   "source": [
    "## Solving the linear model using least-squares\n",
    "Before diving deeper into gradient descent implementation, we solve the linear model using least-squares. This is done to get a reference value to assess covergence and results obtained later. The code below retrieves the training dataset and computes the least-squares solution, applying the `torch.linalg` package ([link](https://pytorch.org/docs/stable/linalg.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2416ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin = training_dataset[:][0]\n",
    "y_lin = training_dataset[:][1]\n",
    "\n",
    "ls_solution = torch.linalg.lstsq(\n",
    "    torch.hstack((x_lin, torch.ones(x_lin.shape[0], 1).to(device))),\n",
    "    y_lin).solution.to('cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0da9f60c",
   "metadata": {},
   "source": [
    "We note the values for weight and bias for later comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef2fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'weight {ls_solution[0].item()}, bias {ls_solution[1].item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba2f253b",
   "metadata": {},
   "source": [
    "We define the start and end points of the least-squares result line. Adding the orange line, representing the result from the least-squares method, to the previous plot, we obtain the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ls_points = torch.FloatTensor([[ DATA_X_MIN, 1], [DATA_X_MAX, 1]])\n",
    "y_ls_values = torch.matmul(x_ls_points, ls_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(FIG_W, FIG_H))\n",
    "\n",
    "x_array, y_real_array, y_noisy_array = training_dataset.as_numpy()\n",
    "\n",
    "x_eval_array, _, y_eval_noisy_array = evaluation_dataset.as_numpy()\n",
    "\n",
    "sns.scatterplot(x=x_array,\n",
    "                y=y_noisy_array,\n",
    "                color=colors['blue'])\n",
    "sns.lineplot(x=x_array,\n",
    "             y=y_real_array,\n",
    "             color=colors['steelblue'], linewidth=2)\n",
    "\n",
    "sns.lineplot(x=x_ls_points[:, 0].numpy(),\n",
    "             y=y_ls_values.squeeze().numpy(),\n",
    "             color=colors['orange'], linewidth=2)\n",
    "\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xticks(range(-1, 4, 1))\n",
    "ax.set_yticks(range(0, 17, 1))\n",
    "sns.despine()\n",
    "set_ax(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9bfc9140",
   "metadata": {},
   "source": [
    "## Defining the Linear Model in PyTorch\n",
    "\n",
    "In this section, we finally approach to the core of things as we define our linear model using the PyTorch library.\n",
    "\n",
    "We create a class called `LinearModel`, which inherits from the `nn.Module` class ([link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)). This class defines a neural network architecture with a single linear layer, which takes one input feature and produces one output. We also provide the option to initialize the weights and biases of the linear layer, if desired. The `forward` method is defined to perform the forward pass through the network.\n",
    "\n",
    "PyTorch provides a convenient way to define a neural network sequentially using `torch.nn.Sequential`. This class allows us to stack multiple layers in sequence, with the output of one layer becoming the input of the next. By passing a list of layer objects as an argument to `Sequential`, we can define a neural network with any number of layers, each with their own activation functions, etc. This approach can save time and effort when designing more complex neural network architectures. However, our current linear regression problem only has one layer with a single input to add to `torch.nn.Sequential`.\n",
    " \n",
    "Note that we do not need to add a bias term explicitly to our linear layer. This is because the bias term can be thought of as an additional input that is always set to 1, and PyTorch's implementation of the linear layer automatically includes this bias input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ad8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, weight_init=None, bias_init=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 1, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "        # Initialize weight and bias if given\n",
    "        if weight_init is not None:\n",
    "            init.constant_(self.model[0].weight, weight_init)\n",
    "        if bias_init is not None:\n",
    "            init.constant_(self.model[0].bias, bias_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d80c1d2",
   "metadata": {},
   "source": [
    "To better visualize the learning process of the gradient descent algorithm, we will initialize our `LinearModel` with non-optimal values. Specifically, we will set the initial weight to `-7` and the initial bias to `7`. This way, we can see how the model learns from these non-optimal starting points to converge towards the true solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8973f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_WEIGHT = -7\n",
    "INITIAL_BIAS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(weight_init=INITIAL_WEIGHT,\n",
    "                    bias_init=INITIAL_BIAS).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42bf07bf",
   "metadata": {},
   "source": [
    "The `named_parameters` method returns an iterator over named parameters of our model, where each parameter is a tuple containing the name of the parameter and its value. In the loop, we print out the name and the value of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a917d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6df339be",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d16dd73d",
   "metadata": {},
   "source": [
    "### Defining loss functions, optimizers, and schedulers\n",
    "To start training our model, we first define the loss function, optimizer, and scheduler with the appropriate learning parameters.\n",
    "\n",
    "For this implementation, we will be using the Mean Squared Error (MSE) loss function [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss). Furthermore, we will be using the Stochastic Gradient Descent (SGD) optimizer [optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD). In each iteration of SGD, the model's parameters are updated based on the gradient of the loss function with respect to those parameters.\n",
    "\n",
    "We define an initial learning rate of `0.05`. Additionally, we define a learning rate scheduler using the [`ExponentialLR`](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR) class in PyTorch, with a decay factor of `0.997`. This scheduler reduces the learning rate by the decay factor `gamma` at each epoch. \n",
    "\n",
    "We will train the model for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb6bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e4187",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ffa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LEARNING_RATE = 0.05\n",
    "GAMMA = 0.997\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(),\n",
    "                            lr=INITIAL_LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer,\n",
    "                                                   gamma=GAMMA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cffb2a9",
   "metadata": {},
   "source": [
    "### Training Initialization and Execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5b6fbb7",
   "metadata": {},
   "source": [
    "After defining the model architecture and the necessary hyperparameters, the next step is to initialize the training process. The `train` function takes in the full monty of inputs, including the model, optimizer, scheduler, random generator, data loaders for training and evaluation, and the number of training epochs. It then executes the training loop for the specified number of epochs.\n",
    "\n",
    "The function then records the training progress by logging the gradient and loss values at each iteration. The function returns two lists of dictionaries: `grad_report` and `loss_report`. The `grad_report` contains the gradient values for the weights and biases of the model, along with the batch loss and learning rate for each iteration. The `loss_report` contains the training and evaluation loss values for each epoch.\n",
    "\n",
    "During training, the model learns to minimize the loss function by adjusting its weights and biases through the [optimizer's backpropagation algorithm](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). The scheduler updates the learning rate at each epoch to ensure that the model converges efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70430aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, optimizer, scheduler, gen, loss_fn,\n",
    "          training_loader, evaluation_loader, epochs):\n",
    "\n",
    "    grad_report = []\n",
    "    loss_report = []\n",
    "\n",
    "    # calculate initial loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "        training_loss = 0\n",
    "\n",
    "        for x_batch, y_batch in training_loader:\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            training_loss += loss\n",
    "\n",
    "        evaluation_loss = 0\n",
    "        for x_eval_batch, y_eval_batch in evaluation_loader:\n",
    "\n",
    "            eval_pred = model(x_eval_batch)\n",
    "            evaluation_loss += loss_fn(eval_pred, y_eval_batch)\n",
    "\n",
    "        loss_report.append({\n",
    "            \"epoch\": -1,\n",
    "            \"training_loss\":\n",
    "                training_loss.item()\n",
    "                * TRAINING_BATCH_SIZE\n",
    "                / TRAINING_SAMPLES,\n",
    "            \"evaluation_loss\":\n",
    "                evaluation_loss.item()\n",
    "                * EVALUATION_BATCH_SIZE\n",
    "                / EVALUATION_SAMPLES,\n",
    "        })\n",
    "\n",
    "    gen.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        training_loss = 0\n",
    "\n",
    "        for batch_idx, (x_batch, y_batch) in enumerate(training_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            training_loss += loss\n",
    "\n",
    "            grad_report.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\":\n",
    "                    epoch\n",
    "                    * TRAINING_SAMPLES\n",
    "                    + batch_idx\n",
    "                    * x_batch.size()[0],\n",
    "                \"weight\": model.model[0].weight.item(),\n",
    "                \"bias\": model.model[0].bias.item(),\n",
    "                \"weight_grad\": model.model[0].weight.grad.item(),\n",
    "                \"bias_grad\": model.model[0].bias.grad.item(),\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"learning_rate\": scheduler.get_last_lr()[0]\n",
    "            })\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            evaluation_loss = 0\n",
    "            for x_eval_batch, y_eval_batch in evaluation_loader:\n",
    "\n",
    "                eval_pred = model(x_eval_batch)\n",
    "                evaluation_loss += loss_fn(eval_pred, y_eval_batch)\n",
    "\n",
    "            loss_report.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"training_loss\":\n",
    "                    training_loss.item()\n",
    "                    * TRAINING_BATCH_SIZE\n",
    "                    / TRAINING_SAMPLES,\n",
    "                \"evaluation_loss\":\n",
    "                    evaluation_loss.item()\n",
    "                    * EVALUATION_BATCH_SIZE\n",
    "                    / EVALUATION_SAMPLES,\n",
    "            })\n",
    "\n",
    "        tr_loss = (training_loss.item()\n",
    "                   * TRAINING_BATCH_SIZE / TRAINING_SAMPLES)\n",
    "        ev_loss = (evaluation_loss.item()\n",
    "                   * EVALUATION_BATCH_SIZE / EVALUATION_SAMPLES)\n",
    "\n",
    "        print(f\"Epoch {epoch:>3}, \"\n",
    "              \"weight \"\n",
    "              f\"{model.model[0].weight.item():.4f}\"\n",
    "              \", bias \"\n",
    "              f\"{model.model[0].bias.item():.4f}\"\n",
    "              \", lr \"\n",
    "              f\"{scheduler.get_last_lr()[0]:.8f}\"\n",
    "              \", training loss: \"\n",
    "              f\"{tr_loss:.4f}\"\n",
    "              \", evaluation loss: \"\n",
    "              f\"{ev_loss:.4f}\")\n",
    "\n",
    "    return grad_report, loss_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03b8a604",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "After defining the necessary components of our data sets and linear model, and initializing the necessary variables, we are finally ready to begin the training process. \n",
    "\n",
    "Using the PyTorch library, we will train our linear model with the defined loss function and optimizer, using the training data, and evaluating the model's performance on the evaluation data. The model will be trained for a specified number of epochs, with the learning rate scheduled to decrease over time using the defined scheduler.\n",
    "\n",
    "The training process will result in an optimized model, ready for use in predicting new values based on the input features - or in this case, a single input feature $x$, using a call `y_pred = model(x_input)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc916641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad_report, loss_report = train(model,\n",
    "                                 optimizer,\n",
    "                                 scheduler,\n",
    "                                 gen,\n",
    "                                 loss_fn,\n",
    "                                 training_loader, evaluation_loader,\n",
    "                                 TRAINING_EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c78b905b",
   "metadata": {},
   "source": [
    "### Visualizing Training and Evaluation Losses\n",
    "\n",
    "We visualize the decreasing trend of loss over the training epochs. The training and evaluation losses are extracted from the `loss_report`, a python `list` of `dict`s, which is returned by the train function. A line plot is created using the Seaborn library, where the x-axis represents the epoch and the y-axis represents the corresponding loss values. The training loss is plotted in blue, while the evaluation loss is plotted in orange. Note that initial loss before any training is done is shown as epoch -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df8f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(FIG_W, FIG_H))\n",
    "\n",
    "epoch = [row['epoch'] for row in loss_report]\n",
    "training_loss = [row['training_loss'] for row in loss_report]\n",
    "evaluation_loss = [row['evaluation_loss'] for row in loss_report]\n",
    "\n",
    "sns.lineplot(x=epoch,\n",
    "             y=training_loss,\n",
    "             color=colors['blue'], linewidth=2)\n",
    "sns.lineplot(x=epoch,\n",
    "             y=evaluation_loss,\n",
    "             color=colors['orange'], linewidth=2)\n",
    "\n",
    "ax = fig.gca()\n",
    "\n",
    "sns.despine()\n",
    "set_ax(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beac05cc",
   "metadata": {},
   "source": [
    "###  Visualization of Linear Approximation during Training\n",
    "\n",
    "The following cell creates a plot that visualizes how the linear approximation $y = wx +b$ changes during training from the initial values towards the near optimal value. The plot consists of several elements:\n",
    "\n",
    "- A scatter plot of the training data points in blue color.\n",
    "- A line plot of the true underlying function (without noise) in light blue color.\n",
    "- Multiple lines represent linear approximations at different training stages. Each line corresponds to a distinct epoch, and the line color gradually transition from deep purple to a pale gold following the previously introduced seaborn flare color palette.\n",
    "- A line plot that represents the final linear approximation at the end of training. This line is in yellow color.\n",
    "- A line plot of the least squares solution in black color.\n",
    "\n",
    " Observe the three outstanding lines corresponding to the initial value of weight and bias (set as -7 and 7, correspondingly) and the two training iterations that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(FIG_W, FIG_H))\n",
    "\n",
    "sns.scatterplot(x=x_array,\n",
    "                y=y_noisy_array,\n",
    "                color=colors['blue'])\n",
    "sns.lineplot(x=x_array,\n",
    "             y=y_real_array,\n",
    "             color=colors['lightblue'], linewidth=2)\n",
    "\n",
    "line_ends = np.array([min(x_array), max(x_array)])\n",
    "\n",
    "for row in grad_report:\n",
    "    weight = row['weight']\n",
    "    bias = row['bias']\n",
    "    sns.lineplot(x=line_ends,\n",
    "                 y=weight*line_ends+bias,\n",
    "                 linewidth=1, alpha=0.2,\n",
    "                 color=palette(row['iter']/grad_report[-1]['iter']))\n",
    "\n",
    "final_weight = grad_report[-1]['weight']\n",
    "final_bias = grad_report[-1]['bias']\n",
    "\n",
    "sns.lineplot(x=line_ends,\n",
    "             y=final_weight*line_ends+final_bias,\n",
    "             color=colors['yellow'], linewidth=2)\n",
    "sns.lineplot(x=x_ls_points[:, 0].numpy(),\n",
    "             y=y_ls_values.squeeze().numpy(),\n",
    "             color='black', linewidth=2)\n",
    "\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xticks(range(-1, 4, 1))\n",
    "ax.set_yticks(range(0, 17, 1))\n",
    "ax.set_xlim(-1, 3)\n",
    "ax.set_ylim(0, 16)\n",
    "sns.despine()\n",
    "set_ax(ax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aac82309",
   "metadata": {},
   "source": [
    "We create a Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) from the dictionary list containing gradient information collected during training as `grad_report`.\n",
    "\n",
    "The displayed dataframe shows the weight and bias values calculated during the training process using the gradient descent algorithm. Each row represents the updated weight and bias values, which are obtained by multiplying the corresponding gradients with the learning rate and subtracting them from the previous values. The dataframe provides an easy way to verify the gradient descent update at each iteration. \n",
    "\n",
    "As can be seen from this small sample, the values of the gradients vary significantly between iterations, and if not compensated with an appropriate learning rate, the weight and bias parameters would be updated uncontrollably.\n",
    "\n",
    "Note that 5 epochs with 200 training samples each gives us 1000 data samples used in model updates in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2786a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_grad_report = pd.DataFrame.from_dict(grad_report)\n",
    "pd_grad_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8375b0fe",
   "metadata": {},
   "source": [
    "### Visualizing the Error Surface\n",
    "\n",
    "In the following, we generate a 3D surface plot using the matplotlib library to visualize the error surface as well as the evolution of gradients during training. \n",
    "\n",
    "For our simple linear model, MSE error values for points of 2D error surface, defined by weight $w$ and bias $b$, and for a given batch of training data, are given by:\n",
    "\n",
    "$$\n",
    "E(w,b) = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - (wx_i+b))^2\n",
    "$$\n",
    "where we calculate the sum over $N$ training batch samples $x_i$ and $y_i$\n",
    "\n",
    "The `get_error_surface()` function takes the training data batch inputs `x_pred` and outputs `y_real` as its inputs and generates an error surface by computing the mean squared error for all combinations of weight and bias values within the defined range, here [-10, 10]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96986317",
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_range = np.linspace(-10, 10, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1196dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_surface(x_pred, y_real):\n",
    "\n",
    "    error_list = []\n",
    "\n",
    "    for w in surf_range:\n",
    "        for b in surf_range:\n",
    "\n",
    "            y_fit = w * x_pred + b\n",
    "            e = 1 / len(x_pred) * sum((y_real - y_fit)**2)\n",
    "            error_list.append([w, b, e])\n",
    "\n",
    "    return pd.DataFrame(data=error_list, columns=['w', 'b', 'e'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb450faf",
   "metadata": {},
   "source": [
    "Here, we calculate the error surface for the whole training data set, using `x_array` and `y_real_array` as inputs. The `z` variable stores the resulting MSE values in a numpy array. \n",
    "\n",
    "Note the use of ground truth values of `y_real_array`, as this error surface is a visualization of the performance of the simple linear model with respect to the true underlying fourth-degree polynomial function, rather than the noisy data used in training.\n",
    "\n",
    "As a sidenote, we highly recommend the Pandas [pivot_table](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot_table.html) method for creating the data arrays for surface plots, such as [Axes3D.plot_trisurf](https://matplotlib.org/stable/api/_as_gen/mpl_toolkits.mplot3d.axes3d.Axes3D.plot_trisurf.html) and [matplotlib.pyplot.contourf](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contourf.html) used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b558e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = get_error_surface(x_array, y_real_array)\n",
    "z = error.pivot_table(index='w', columns='b', values='e').T.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81753608",
   "metadata": {},
   "source": [
    "The resulting 3D plot shows the error surface for the whole training data set as a trisurface and uses our predefined cubehelix colormap `cm`, to indicate the magnitude of the MSE for each pair of weight and bias. The `azim` parameter adjusts the viewing angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dc3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = plt.axes(projection='3d', azim=-35)\n",
    "ax.plot_trisurf(error['w'], error['b'], error['e'],\n",
    "                cmap=cm, linewidth=0.2)\n",
    "\n",
    "ax.set_xlabel(\"w\", fontsize=14)\n",
    "ax.set_ylabel(\"b\", fontsize=14)\n",
    "ax.set_zlabel(\"MSE\", fontsize=14)\n",
    "\n",
    "ax.set_xticks(range(-10,11,2))\n",
    "ax.set_yticks(range(-10,11,2))\n",
    "\n",
    "\n",
    "set_ax(ax)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca369db2",
   "metadata": {},
   "source": [
    "### Visualization of Gradient Descent Trajectory on Error Surface\n",
    "\n",
    "The following code visualizes the gradient descent trajectory on the error surface. \n",
    "\n",
    "The error surface is represented by a contour plot, with contour lines indicating the levels of the mean squared error. The contour plot is created using the previously calculated values of `z`, which is the mean squared error calculated for different combinations of weight and bias values using all of the training data to evaluate the error.\n",
    "\n",
    "The code also plots the gradient descent trajectory on the contour plot, showing how the values of weight and bias evolve during training, from one update to another. The trajectory is visualized using a line plot, with the color of the line indicating the iteration: The initial `VIS_STEPS` iterations of weight and bias are represented by orange line and markers, while the rest of the training iterations are shown in light blue. The least-squares solution for weight and bias calculated earlier in this notebook is represented by a red cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f14081",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTOUR_LEVELS = 20\n",
    "VIS_STEPS = 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d75d397f",
   "metadata": {},
   "source": [
    "These are the initial weight and bias values we aim to highlight in the plot. Once again, it is worth noting the three initial lines with outstanding weight and bias values, and the large gradient value that follows on the third line, resulting in a marked adjustment of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cca320",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_grad_report[0:VIS_STEPS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9167a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.contourf(surf_range, surf_range, z,\n",
    "             cmap=cm, levels=CONTOUR_LEVELS)  \n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.lineplot(pd_grad_report,\n",
    "             x='weight',\n",
    "             y='bias',\n",
    "             color=colors['lightblue'], linewidth=2,\n",
    "             sort=False)\n",
    "sns.lineplot(pd_grad_report[0:VIS_STEPS],\n",
    "             x='weight',\n",
    "             y='bias',\n",
    "             color=colors['orange'], linewidth=2,\n",
    "             sort=False)\n",
    "sns.scatterplot(pd_grad_report[0:VIS_STEPS],\n",
    "                x='weight',\n",
    "                y='bias',\n",
    "                color=colors['orange'], marker='o',\n",
    "                s=12, zorder=100)\n",
    "plt.plot(ls_solution[0].item(), ls_solution[1].item(), 'x',\n",
    "         color=colors['red'], markersize=10, markeredgewidth=4)\n",
    "\n",
    "ax.set_xticks(range(-10, 11, 2))\n",
    "ax.set_yticks(range(-10, 11, 2))\n",
    "\n",
    "ax.set_xlabel(\"w\", fontsize=16)\n",
    "ax.set_ylabel(\"b\", fontsize=16, rotation=0)\n",
    "\n",
    "set_ax(ax)\n",
    "sns.despine()\n",
    "plt.axis('square')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a6179de",
   "metadata": {},
   "source": [
    "### Visualizing the Gradient Descent for Early Training Iterations\n",
    "\n",
    "In the following, we plot the gradient and error surface for the first few training iterations. We illustrate how the form of the error surface and consequently the direction of steepest descent depend on the value of the current training sample or values of the current training batch. \n",
    "\n",
    "By examining the changes in the error surface and the direction of the gradient as the training progresses, we hope to gain a better understanding of how the algorithm learns to optimize the model parameters to minimize the loss.\n",
    "\n",
    "The following function, `plot_first_gradients`, plots the gradient and changing error surface for the `n_steps` training iterations of the gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80496f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_gradients(report, loader, n_steps):\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(16, 16),\n",
    "                             sharex=True, sharey=True,\n",
    "                             subplot_kw={'box_aspect': 1,\n",
    "                                         'xticks': [-10, -5, 0, 5, 10],\n",
    "                                         'yticks': [-10, -5, 0, 5, 10]})\n",
    "    axes = axes.flatten('C')\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        for x_batch, y_batch in loader:\n",
    "\n",
    "            if i == n_steps:\n",
    "                return\n",
    "\n",
    "            x_bn = x_batch.squeeze(-1).to('cpu').detach().numpy()\n",
    "            y_bn = y_batch.squeeze(-1).to('cpu').detach().numpy()\n",
    "\n",
    "            n_samples = 1\n",
    "            error = get_error_surface(x_bn, y_bn)\n",
    "            z = error.pivot_table(\n",
    "                index='w', columns='b', values='e').T.values\n",
    "\n",
    "            ax = axes[i]\n",
    "\n",
    "            ax.contourf(surf_range, surf_range, z,\n",
    "                        cmap=cm, levels=CONTOUR_LEVELS)\n",
    "            sns.scatterplot(report[0:i+1],\n",
    "                            x='weight',\n",
    "                            y='bias',\n",
    "                            color=colors['orange'], marker='o',\n",
    "                            s=12, zorder=100, ax=ax)\n",
    "            sns.lineplot(report[0:i+1],\n",
    "                         x='weight',\n",
    "                         y='bias',\n",
    "                         color=colors['orange'], linewidth=2,\n",
    "                         sort=False, ax=ax)\n",
    "            sns.lineplot(report[i:i+2],\n",
    "                         x='weight',\n",
    "                         y='bias',\n",
    "                         color=colors['red'], linewidth=2,\n",
    "                         sort=False, ax=ax)\n",
    "\n",
    "            point_x = report.iloc[i].weight\n",
    "            point_y = report.iloc[i].bias\n",
    "\n",
    "            grad_len = np.linalg.norm(\n",
    "                report.iloc[i][['weight_grad', 'bias_grad']])\n",
    "            grad_scale = 3\n",
    "\n",
    "            grad_x = - report.iloc[i].weight_grad / \\\n",
    "                grad_len * grad_scale\n",
    "            grad_y = - report.iloc[i].bias_grad / grad_len * grad_scale\n",
    "\n",
    "            ax.arrow(point_x, point_y, grad_x, grad_y,\n",
    "                     length_includes_head=True,\n",
    "                     color=colors['lightblue'],\n",
    "                     linewidth=2, head_width=0.4, head_length=0.4)\n",
    "\n",
    "            ax.set_xlim(-10, 10)\n",
    "            ax.set_ylim(-10, 10)\n",
    "            sns.despine()\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e72e2b4",
   "metadata": {},
   "source": [
    "The following subplots show the error surface contour map, calculated for each current training batch of size one. Remember, we set `TRAINING_BATCH_SIZE = 1` and consequently, model parameters were updated for each training sample. Note that we again set the manual seed of the Generator to ensure that the training data loader provides batches in the same order as during model training, as recorded in the `grad_report`.\n",
    "\n",
    "The plots show the evolution of the weight $w$ and bias $b$ over iterations in orange, and the current gradient direction as a light blue arrow. As the length of the arrow is normalized, the red line indicates the magnitude of the gradient.\n",
    "\n",
    "These successive images reveal that the error surface and consequently the gradient, indicating direction of the steepest descent, changes significantly and unpredictably from one iteration to another, sometimes even pointing to opposite directions. It is worth noting how the error surface is very flat around our initial values for the first two updates, resulting only in small updates, but significantly larger for the third update. This highlights the instability of the optimization process and the sensitivity of the gradient to the training data.\n",
    "\n",
    "However, despite the instability of the optimization process, it is worth noting that the algorithm is still able to converge towards the optimal solution. This underlines the robustness of the gradient descent algorithm and helps to explain the algorithm's prominent role in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a36a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.manual_seed(RANDOM_SEED)\n",
    "plot_first_gradients(pd_grad_report, training_loader, VIS_STEPS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c2fcb69",
   "metadata": {},
   "source": [
    "## Training with Increased Batch Size\n",
    "We now repeat the previous model training, but with a training batch size of 10 instead of 1. This means that the model will be trained on 10 samples at a time before updating the weight and bias values. We will again visualize the error surface to observe any differences in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_BATCH_SIZE = 10\n",
    "\n",
    "gen2 = torch.Generator()\n",
    "gen2.manual_seed(RANDOM_SEED)\n",
    "\n",
    "training_loader_batch = DataLoader(training_dataset,\n",
    "                                   batch_size=TRAINING_BATCH_SIZE,\n",
    "                                   shuffle=True, generator=gen2)\n",
    "model2 = LinearModel(weight_init=INITIAL_WEIGHT,\n",
    "                     bias_init=INITIAL_BIAS).to(device)\n",
    "\n",
    "optimizer2 = torch.optim.SGD(\n",
    "    model2.parameters(), lr=INITIAL_LEARNING_RATE)\n",
    "scheduler2 = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer2, gamma=GAMMA)\n",
    "\n",
    "gen2.manual_seed(RANDOM_SEED)\n",
    "grad_report_batch, loss_report_batch = train(model2,\n",
    "                                             optimizer2,\n",
    "                                             scheduler2,\n",
    "                                             gen2,\n",
    "                                             loss_fn,\n",
    "                                             training_loader_batch,\n",
    "                                             evaluation_loader,\n",
    "                                             TRAINING_EPOCHS)\n",
    "\n",
    "pd_grad_report_batch = pd.DataFrame.from_dict(grad_report_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84e8888e",
   "metadata": {},
   "source": [
    "Let's take a peek at our grad_report dataframe. Note that after initial `VIS_STEPS` iterations, the algorithm has been exposed to `TRAINING_BATCH_SIZE * VIS_STEPS` individual training data samples as indicated by the `iter` column of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe70a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_grad_report_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4f87569",
   "metadata": {},
   "source": [
    "### Convergence of Gradient Descent Algorithm\n",
    "\n",
    "Before we dive into visualizing the error surface, let's take a sidestep to see how the algorithm is converging and check if increasing the batch size has made any difference in performance.\n",
    "\n",
    "The following code generates a plot to compare the training and evaluation loss over epochs for two different batch sizes: previously used single observation updates with batch size of one and current batch size of 10. The plot shows previous results for single observation updates in light color, blue for training lossa and orange for evaluation loss. The loss report lists are iterated to obtain the loss values for the two different batch sizes.\n",
    "\n",
    "As the lines mostly overlap, it appears that there is no significant difference in learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e55168",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(FIG_W, FIG_H))\n",
    "\n",
    "epoch = [row['epoch'] for row in loss_report]\n",
    "training_loss = [row['training_loss'] for row in loss_report]\n",
    "evaluation_loss = [row['evaluation_loss'] for row in loss_report]\n",
    "\n",
    "epoch_batch = [row['epoch'] for row in loss_report_batch]\n",
    "training_loss_batch = [row['training_loss']\n",
    "                       for row in loss_report_batch]\n",
    "evaluation_loss_batch = [row['evaluation_loss']\n",
    "                         for row in loss_report_batch]\n",
    "\n",
    "sns.lineplot(x=epoch, y=training_loss,\n",
    "             color=colors['lightblue'], linewidth=2)\n",
    "sns.lineplot(x=epoch, y=evaluation_loss,\n",
    "             color=colors['lightorange'], linewidth=2)\n",
    "\n",
    "sns.lineplot(x=epoch_batch, y=training_loss_batch,\n",
    "             color=colors['blue'], linewidth=2)\n",
    "sns.lineplot(x=epoch_batch, y=evaluation_loss_batch,\n",
    "             color=colors['orange'], linewidth=2)\n",
    "\n",
    "ax = fig.gca()\n",
    "set_ax(ax)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bc7c5dd",
   "metadata": {},
   "source": [
    "### Comparing Results for Single Observation and Batch Training: Evolution of Distance to Least Squares Estimate\n",
    "We compare resuls for single observation and batch training by plotting the evoluton of the distance between the parameter estimates and the least squares estimate of the model parameters during training. The distance is calculated as the $\\mathcal{L}1$ norm or Manhattan distance between the estimated weight and bias values obtained through gradient descent, and the weight and bias estimates obtained through the least squares solution. \n",
    "\n",
    "The plot shows how the distance to optimal value decreases as the algorithm iteratively updates the weight and bias parameters during training. On the left, for the batch size of one, and on the right for batch size of ten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73707293",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_grad_report['dist'] = (pd_grad_report[['weight',\n",
    "                                          'bias']]\n",
    "                          - [ls_solution[0].item(),\n",
    "                             ls_solution[1].item()]\n",
    "                          ).abs().sum(axis=1)\n",
    "pd_grad_report_batch['dist'] = (pd_grad_report_batch[['weight',\n",
    "                                                      'bias']] \n",
    "                                - [ls_solution[0].item(),\n",
    "                                   ls_solution[1].item()]\n",
    "                                ).abs().sum(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4311d423",
   "metadata": {},
   "source": [
    "Comparing the distance for the final tenth of iterations, we see a slight difference in mean distance, visible also in plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f7f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 900\n",
    "\n",
    "final_dist_single = np.mean(\n",
    "    (pd_grad_report[\n",
    "        pd_grad_report['iter'] >= limit]['dist']))\n",
    "final_dist_batch = np.mean(\n",
    "    (pd_grad_report_batch[\n",
    "        pd_grad_report_batch['iter'] >= limit]['dist']))\n",
    "\n",
    "print(f\"Single sample:   {final_dist_single:.4f}\")\n",
    "print(f\"Batch trainging: {final_dist_batch:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc45e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 16),\n",
    "                         sharex=True, sharey=True,\n",
    "                         subplot_kw={'box_aspect': 3/4})\n",
    "\n",
    "ax = axes[0]\n",
    "\n",
    "sns.lineplot(pd_grad_report,\n",
    "             x='iter',\n",
    "             y='dist',\n",
    "             color=colors['blue'], linewidth=2,\n",
    "             ax=ax)\n",
    "\n",
    "ax.set_title('Batch size 1', y=0.9)\n",
    "set_ax(ax)\n",
    "\n",
    "ax = axes[1]\n",
    "\n",
    "sns.lineplot(pd_grad_report_batch,\n",
    "             x='iter',\n",
    "             y='dist',\n",
    "             color=colors['blue'], linewidth=2,\n",
    "             ax=ax)\n",
    "\n",
    "ax.set_title('Batch size 10', y=0.9)\n",
    "set_ax(ax)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9aa810e",
   "metadata": {},
   "source": [
    "### Batch training gradient direction and error surface fluctuations\n",
    "\n",
    "Below, we recreate the error surface contour maps for the initial updates using the current training batch size of ten samples, calculated at each iteration. \n",
    "\n",
    "The error surface for all of the training set is represented by a contour plot. The trajectory is visualized using a line plot, with the initial `VIS_STEPS` iterations represented by orange line and markers, while the rest of the training iterations are shown in light blue. The least-squares solution is represented by a red cross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf6b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "\n",
    "plt.contourf(surf_range, surf_range, z,\n",
    "             cmap=cm, levels=CONTOUR_LEVELS)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "sns.lineplot(pd_grad_report_batch,\n",
    "             x='weight',\n",
    "             y='bias',\n",
    "             color=colors['lightblue'], linewidth=2,\n",
    "             sort=False)\n",
    "sns.lineplot(pd_grad_report_batch[0:VIS_STEPS],\n",
    "             x='weight',\n",
    "             y='bias',\n",
    "             color=colors['orange'], linewidth=2,\n",
    "             sort=False)\n",
    "sns.scatterplot(pd_grad_report_batch[0:VIS_STEPS],\n",
    "                x='weight',\n",
    "                y='bias',\n",
    "                color=colors['orange'],\n",
    "                marker='o', s=12, zorder=100)\n",
    "plt.plot(ls_solution[0].item(), ls_solution[1].item(), 'x',\n",
    "         color=colors['red'], markersize=10, markeredgewidth=4)\n",
    "\n",
    "ax.set_xticks(range(-10, 11, 2))\n",
    "ax.set_yticks(range(-10, 11, 2))\n",
    "\n",
    "ax.set_xlabel(\"w\", fontsize=16)\n",
    "ax.set_ylabel(\"b\", fontsize=16, rotation=0)\n",
    "\n",
    "set_ax(ax)\n",
    "sns.despine()\n",
    "plt.axis('square')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0af8fced",
   "metadata": {},
   "source": [
    "Following subplots show the the error surface calculated for each training batch for the first iterations. Current parameter values, and gradient directon and magnitude are indicated in each subplot as previously.\n",
    "\n",
    "As explained, in contrast to updating model values based on single data samples, batch training involves updating the model values based on a batch of multiple data samples at once. This approach reduces the fluctuations of the error surface that can arise from using a single sample and provides a more stable estimate of the gradient direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05211802",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2.manual_seed(RANDOM_SEED)\n",
    "plot_first_gradients(pd_grad_report_batch,\n",
    "                     training_loader_batch, VIS_STEPS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3326f5b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have explored the implementation of stochastic gradient descent (SGD) using PyTorch with a simple linear model. Here are some final thoughts on what we have learned.\n",
    "\n",
    "First, PyTorch is a powerful and flexible tool for building and training machine learning models. Using PyTorch to implement SGD with a simple linear model is an excellent way to learn how to code in PyTorch. Using [torch.autograd](https://pytorch.org/docs/stable/autograd.html) of Pytorch, while not elaborated in more detail here, makes the implementation of gradient-based algorithms, such as stochastic gradient descent, really simple. Autograd allows us to automatically compute the gradients of the loss function with respect to the model parameters. This was effectively achieved in three lines of code in this notebook.\n",
    "\n",
    "We have also learned how to use datasets and loaders in PyTorch, making the implementation useful for real training data. In addition, the built-in code to leverage a CUDA-enabled graphics processing unit (GPU) can be useful when working with larger problems, allowing us to take advantage of the speed and parallelism offered by GPUs of cloud computing platforms with little modification. \n",
    "\n",
    "Overall, the required modifications to use the code for more complex machine learning problems are modest. To apply the code to a more realistic machine learning problem, we would need to extend our linear single-layer neural network architecture to contain multiple hidden layers with nonlinearities. We would also need to adjust the input and output dimensions of the model to match the data. Additionally, we might need to modify the loss function to suit the specific problem we are trying to solve. For example, if we are working on a classification problem, we may want to use cross-entropy loss instead of mean squared error.\n",
    "\n",
    "Secod, we have gained experience on how to apply Python vizualization libraries, and in particular, how to create visualizations of the error surfaces of SGD. This has proven useful for understanding how the algorithm works and how it navigates the parameter space to find optimal solutions. We have shown how SGD is sensitive to the choice of the training batch size: A too small batch size can lead to a noisy estimate of the gradients, while a large batch size can make the algorithm converge slowly in a realistic problem setting and may even get stuck in a bad local minimum when the error surface is more varied.\n",
    "\n",
    "Another challenge we have not explored here in detail, is the choice of the learning rate, which determines the step size of the updates. We have seen that even in our simple example, there are large differences in gradient magnitudes. A too large learning rate can lead to oscillations or divergence of the algorithm, while a too small learning rate can result in slow convergence or getting stuck in a local minimum. Therefore, it is important to tune the learning rate carefully to obtain a good trade-off between convergence speed and stability. Future work to investigate and visualize the effect of the learning rate choice can be based on the implementation available in this notebook,\n",
    "\n",
    "So, while SGD is a powerful and widely used optimization algorithm, it presents some challenges that need to be addressed to obtain good performance. By carefully tuning the learning rate, the mini-batch size, and applying regularization techniques, SGD can be a reliable and effective optimization method for training deep neural networks as well.\n",
    "\n",
    "This notebook has introduced some of the key concepts and techniques in machine learning. By understanding these concepts, we can build and train more sophisticated models for various problems. We hope this notebook has been useful for learning about the concepts and serves as a helpful starting point for exploring advanced machine learning topics. \n",
    "\n",
    "Please note that the while text and equations have been generated with ChatGPT's assistance, the author takes full responsibility for the content and any errors or omissions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
